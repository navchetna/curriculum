{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNW9GID9bT5x+xpGTf8RHj5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Oy4RE9U2LbiB"},"outputs":[],"source":["!pip install git+https://github.com/openai/whisper.git\n","\n","import whisper\n","\n","model = whisper.load_model(\"base\")\n","result = model.transcribe(\"audio.mp3\")\n","print(result[\"text\"])\n"]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n","\n","# Install necessary libraries if not already installed\n","!pip install transformers datasets\n","\n","# Load the model and processor\n","model_id = \"openai/whisper-base\"  # Or any other whisper model\n","model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id)\n","processor = AutoProcessor.from_pretrained(model_id)\n","\n","# Move the model to the GPU if available\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model.to(device)\n","\n","\n","# Load audio file\n","# Replace with your actual audio file path\n","audio_filepath = \"audio.mp3\"\n","\n","# Use librosa or other library to load audio if necessary\n","import librosa\n","audio_input, sr = librosa.load(audio_filepath)\n","\n","# Preprocess the audio\n","inputs = processor(audio_input, sampling_rate=sr, return_tensors=\"pt\").to(device)\n","\n","# Perform inference\n","with torch.no_grad():\n","  generated_ids = model.generate(**inputs)\n","\n","# Decode the generated IDs to text\n","transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","\n","# Print the transcribed text\n","transcription\n"],"metadata":{"id":"LQdDxj1IMntl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import whisper\n","import torch\n","from transformers import pipeline\n","\n","# Load the Whisper model\n","model = whisper.load_model(\"base\")\n","\n","# Create a pipeline for transcription\n","pipe = pipeline(\"automatic-speech-recognition\", model=model)\n","\n","# Transcribe the audio file\n","result = pipe(\"audio.mp3\")\n","\n","# Print the transcribed text\n","print(result[\"text\"])\n"],"metadata":{"id":"Euk3KBJXRzHb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install faster-whisper\n","\n","from faster_whisper import WhisperModel\n","\n","model_size = \"large-v2\"\n","\n","# Run on GPU with FP16\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = WhisperModel(model_size, device=device, compute_type=\"float16\")\n","\n","# or run on GPU with INT8\n","# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n","# or run on CPU with INT8\n","# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n","\n","segments, info = model.transcribe(\"audio.mp3\", beam_size=5)\n","\n","print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n","\n","for segment in segments:\n","    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n"],"metadata":{"id":"E7ibeNjzN0dG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers datasets librosa soundfile\n","\n","import torch\n","from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\n","import librosa\n","\n","# Load the model and processor\n","model_id = \"facebook/seamless_m4t_large\"  # Or another SeamlessM4T model\n","model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","processor = AutoProcessor.from_pretrained(model_id)\n","\n","# Load audio file\n","audio_filepath = \"audio.mp3\"\n","audio_input, sr = librosa.load(audio_filepath, sr=16000) # Ensure consistent sample rate\n","\n","# Preprocess the audio\n","inputs = processor(audio_input, sampling_rate=sr, return_tensors=\"pt\").to(model.device)\n","\n","\n","# Perform inference\n","with torch.no_grad():\n","    generated_ids = model.generate(**inputs)\n","\n","# Decode the generated IDs to text\n","transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","\n","transcription\n"],"metadata":{"id":"12Pel4WyOSFF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\n","import librosa\n","\n","# Load the model and processor\n","model_id = \"facebook/wav2vec2-base-960h\"  # Replace with your desired Wav2Vec 2.0 model\n","model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","processor = AutoProcessor.from_pretrained(model_id)\n","\n","# Load audio file\n","audio_filepath = \"audio.mp3\"  # Replace with your audio file path\n","audio_input, sr = librosa.load(audio_filepath, sr=16000) # Ensure consistent sample rate\n","\n","# Preprocess the audio\n","inputs = processor(audio_input, sampling_rate=sr, return_tensors=\"pt\").to(model.device)\n","\n","# Perform inference\n","with torch.no_grad():\n","    generated_ids = model.generate(**inputs)\n","\n","# Decode the generated IDs to text\n","transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","\n","transcription\n"],"metadata":{"id":"thIBcpHuPhg9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import pipeline\n","\n","model_path = \"hindi_models/whisper-medium-hi_alldata_multigpu\"\n","device = \"cuda\"\n","lang_code = \"hi\"\n","\n","whisper_asr = pipeline(\n","    \"automatic-speech-recognition\", model=model_path, device=device,\n",")\n","\n","# Special case to handle odia since odia is not supported by whisper model\n","if lang_code == 'or':\n","    whisper_asr.model.config.forced_decoder_ids = (\n","        whisper_asr.tokenizer.get_decoder_prompt_ids(\n","            language=None, task=\"transcribe\"\n","        )\n","    )\n","else:\n","    whisper_asr.model.config.forced_decoder_ids = (\n","        whisper_asr.tokenizer.get_decoder_prompt_ids(\n","            language=lang_code, task=\"transcribe\"\n","        )\n","    )\n","\n","result = whisper_asr(\"audio.mp3\")\n","print(result[\"text\"])"],"metadata":{"id":"1FHrze90P55a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"J0TBjXfMQg7n"},"execution_count":null,"outputs":[]}]}